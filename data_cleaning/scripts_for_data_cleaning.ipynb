{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configures logging to a file, overwriting it on each run.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        filename=config.LOG_FILE,\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        filemode='w',\n",
    "        force=True\n",
    "    )\n",
    "\n",
    "def create_db_engine():\n",
    "    \"\"\"Creates a database engine from config settings.\"\"\"\n",
    "    try:\n",
    "        connection_url = (\n",
    "            f'postgresql+psycopg2://{config.DB_USER}:{config.DB_PASSWORD}@'\n",
    "            f'{config.DB_HOST}:{config.DB_PORT}/{config.DB_NAME}'\n",
    "        )\n",
    "        engine = create_engine(connection_url)\n",
    "        engine.connect()\n",
    "        logging.info(\"Successfully connected to PostgreSQL database.\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"PostgreSQL connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_to_postgres(df, table_name, engine):\n",
    "    \"\"\"Loads a DataFrame into a specified PostgreSQL table.\"\"\"\n",
    "    try:\n",
    "        df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "        logging.info(f\"Successfully loaded data into table '{table_name}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save data to table '{table_name}': {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ETL Process Finished in 3 minutes and 1.58 seconds. ---\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main ETL process function.\"\"\"\n",
    "    start_time = time.time()\n",
    "    setup_logging()\n",
    "    logging.info(\"--- Starting SEQ Bus Data ETL Process ---\")\n",
    "\n",
    "    engine = create_db_engine()\n",
    "    if not engine:\n",
    "        logging.critical(\"Could not establish database connection. Aborting.\")\n",
    "        return \n",
    "\n",
    "    base_dir = config.GTFS_DATA_PATH\n",
    "\n",
    "    files_to_process = {\n",
    "        'calendar_dates.txt': 'calendar_dates',\n",
    "        'calendar.txt': 'calendar',\n",
    "        'routes.txt': 'routes',\n",
    "        'shapes.txt': 'shapes',\n",
    "        'stop_times.txt': 'stop_times',\n",
    "        'stops.txt': 'stops',\n",
    "        'trips.txt': 'trips'\n",
    "    }\n",
    "    \n",
    "    for file_name, table_name in files_to_process.items():\n",
    "        try:\n",
    "            file_path = f\"{base_dir}/{file_name}\"\n",
    "            \n",
    "            logging.info(f\"Reading {file_name} from disk...\")\n",
    "            df = pd.read_csv(file_path, low_memory=False)\n",
    "            load_to_postgres(df, table_name, engine)\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"File not found: {file_path}. Skipping.\")\n",
    "        except MemoryError:\n",
    "            logging.error(f\"OUT OF MEMORY while loading {file_name}. Aborting process.\")\n",
    "            break \n",
    "        except Exception as e:\n",
    "            logging.error(f\"An ETL error occurred for {file_name}. Skipping. Details: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_seconds = end_time - start_time\n",
    "    minutes, seconds = divmod(total_seconds, 60)\n",
    "\n",
    "    final_message = (\n",
    "        f\"--- ETL Process Finished in {int(minutes)} minutes and {seconds:.2f} seconds. ---\"\n",
    "        )\n",
    "    logging.info(final_message)\n",
    "    print(final_message)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_db_engine()\n",
    "\n",
    "calendar_dates = pd.read_sql_table('calendar_dates', engine)\n",
    "calendar = pd.read_sql_table('calendar', engine)\n",
    "routes= pd.read_sql_table('routes', engine)\n",
    "shapes = pd.read_sql_table('shapes', engine)\n",
    "stop_times = pd.read_sql_table('stop_times', engine)\n",
    "stops = pd.read_sql_table('stops', engine)\n",
    "trips = pd.read_sql_table('trips', engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    'calendar_dates': calendar_dates,\n",
    "    'calendar': calendar,\n",
    "    'routes': routes,\n",
    "    'shapes': shapes,\n",
    "    'stop_times': stop_times,\n",
    "    'stops': stops,\n",
    "    'trips': trips\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar_dates: 0\n",
      "calendar: 0\n",
      "routes: 0\n",
      "shapes: 0\n",
      "stop_times: 0\n",
      "stops: 0\n",
      "trips: 0\n"
     ]
    }
   ],
   "source": [
    "for name, df in dfs.items():\n",
    "    name_duplicated = df.duplicated().sum()\n",
    "    print(f'{name}: {name_duplicated}')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><strong><center>DATA CLEANING</h2></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing Irrelevent Values\n",
    "logging.info(\"Cleaning 'routes' table\")\n",
    "routes['route_desc'].fillna('No Description', inplace=True)\n",
    "\n",
    "logging.info(\"Cleaning 'routes' table\")\n",
    "stops['stop_code'].fillna('0', inplace=True)\n",
    "stops['stop_desc'].fillna('No Description', inplace=True)\n",
    "stops['zone_id'].fillna(0, inplace=True)\n",
    "stops['stop_url'].fillna('Not Available', inplace=True)\n",
    "stops['platform_code'].fillna('N/A', inplace=True)\n",
    "\n",
    "#Data Type Conversion for calendar Table\n",
    "logging.info(\"Convert  'calendar' date column\")\n",
    "calendar['start_date'] = pd.to_datetime(calendar['start_date'], format='%Y%m%d')\n",
    "calendar['end_date'] = pd.to_datetime(calendar['end_date'], format='%Y%m%d')\n",
    "\n",
    "logging.info(\"Convert  'calendar_dates' date column\")\n",
    "calendar_dates['date'] = pd.to_datetime(calendar_dates['date'], format='%Y%m%d')\n",
    "\n",
    "#Checking Duplicates\n",
    "logging.info('Checking all tables for duplicate row')\n",
    "for name, df in dfs.items():\n",
    "    num_duplicate = df.duplicated().sum()\n",
    "    if num_duplicate > 0:\n",
    "        logging.warning(f'Found {num_duplicate} duplicate rows in {name}. Removing Them')\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    else:\n",
    "        logging.info(f'No Duplicate rows found in {name}')\n",
    "\n",
    "#Removing whitespace from routes_df\n",
    "logging.info(\"Trimming Whitespaces from 'route_df' columns\")\n",
    "routes['route_short_name'] = routes['route_short_name'].str.strip()\n",
    "routes['route_long_name'] = routes['route_long_name'].str.strip()\n",
    "\n",
    "#Removing whitespace from stops_df\n",
    "logging.info(\"Trimming Whitespaces from 'stops_df' columns\")\n",
    "stops['stop_name'] = stops['stop_name'].str.strip()\n",
    "\n",
    "#pushing back to Postgres after cleaning data\n",
    "logging.info(\"Pushing all modified data back to PostgreSQL...\")\n",
    "\n",
    "engine = create_db_engine()\n",
    "\n",
    "if engine:\n",
    "    try:\n",
    "        for table_name, df in dfs.items():\n",
    "            df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "        logging.info('All cleaned tables have been successfully pushed to the database')\n",
    "    except Exception as e:\n",
    "        error_message = (f'Error Occured during the database push: {e}')\n",
    "        print(error_message)\n",
    "        logging.error(error_message)\n",
    "else:\n",
    "    connection_error_message = \"Database connection failed. Could not push data.\"\n",
    "    print(connection_error_message)\n",
    "    logging.critical(connection_error_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
